{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7H/3E82HvCkvE1m2YqzAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinkun21/Recurrent_Neural_Network/blob/master/Notebook_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G11yhI9ARpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "0eaf49ac-9aab-443d-bd63-85ad63b9d131"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV4aN-OI1pb9",
        "colab_type": "text"
      },
      "source": [
        "#### Step 1: Getting used to text data\n",
        "In this exercise, you will play with text data by analyzing quotes from Sheldon Cooper in The Big Bang Theory TV show. This will give you a chance to analyze sentences to obtain insights on what it's like to deal with real-world text data.\n",
        "\n",
        "You will use dictionary comprehensions to create dictionaries that map words to indexes and vice versa. The use of dictionaries instead of, for example, a pandas.DataFrame is because they are more intuitive and don't add unnecessary extra complexity.\n",
        "\n",
        "The data is available in sheldon_quotes with the first two sentences already printed for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWltFkFL_e7G",
        "colab_type": "code",
        "outputId": "1b2c8363-a41b-493e-ad21-127ba139b77b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sheldon_quotes = [\"You're afraid of insects and women, Ladybugs must render you catatonic.\", 'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.', 'For example, I cry because others are stupid, and that makes me sad.', \"I'm not insane, my mother had me tested.\", 'Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.', \"Amy's birthday present will be my genitals.\", '(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!', 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.', 'I would have been here sooner but the bus kept stopping for other people to get on it.', 'Oh gravity, thou art a heartless bitch.', 'I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.', 'Well, today we tried masturbating for money.', 'I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.', \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\", \"What computer do you have? And please don't say a white one.\", \"She calls me moon-pie because I'm nummy-nummy and she could just eat me up.\", 'Ah, memory impairment; the free prize at the bottom of every vodka bottle.']\n",
        "len(sheldon_quotes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuRSH7W9_mNF",
        "colab_type": "text"
      },
      "source": [
        "1. join the sentences into one variable and then extract all the words and store this list in all_words.\n",
        "2. Remove the duplicated words by applying list(set()) on the list of words and store them in unique_words.\n",
        "3. Create a dictionary with indexes as keys and words as values using dictionary comprehensions.\n",
        "4. Create a dictionary with words as keys and indexes as values using dictionary comprehensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2NWdhCu_ni4",
        "colab_type": "code",
        "outputId": "caf991a3-2ba3-4aa6-b960-42edec7416d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Transform the list of sentences into a list of words\n",
        "all_words = ' '.join(sheldon_quotes).split(' ')\n",
        "\n",
        "# Get number of unique words\n",
        "unique_words = list(set(all_words))\n",
        "\n",
        "# Dictionary of indexes as keys and words as values\n",
        "index_to_word = {i+1:wd for i, wd in enumerate(sorted(unique_words))}\n",
        "\n",
        "\n",
        "# Dictionary of words as keys and indexes as values\n",
        "word_to_index = {wd:i+1 for i,wd in enumerate(sorted(unique_words))}\n",
        "\n",
        "# Index 0 for unknown words\n",
        "word_to_index['UKN']=0\n",
        "index_to_word[0]= 'UKN'\n",
        "\n",
        "print(\"index_to_word- \",index_to_word)\n",
        "print(\"\")\n",
        "print(\"Word to Index- \",word_to_index)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index_to_word-  {1: '(3', 2: 'Ah,', 3: \"Amy's\", 4: 'And', 5: 'Explorer', 6: 'Firefox.', 7: 'For', 8: 'Galileo,', 9: 'Goblin', 10: 'Green', 11: 'Hubble', 12: 'I', 13: \"I'm\", 14: 'Internet', 15: 'Ladybugs', 16: 'Oh', 17: 'Paul', 18: 'Penny', 19: 'Penny!', 20: 'Pope', 21: 'Scissors', 22: 'She', 23: 'Spider-Man,', 24: 'Spock', 25: 'Spock,', 26: 'Thankfully', 27: 'The', 28: 'Two', 29: 'V', 30: 'Well,', 31: 'What', 32: 'Wheaton!', 33: 'Wil', 34: \"You're\", 35: 'a', 36: 'afraid', 37: 'all', 38: 'always', 39: 'am', 40: 'and', 41: 'appeals', 42: 'are', 43: 'art', 44: 'as', 45: 'at', 46: 'aware', 47: 'based', 48: 'be', 49: 'became', 50: 'because', 51: 'been', 52: 'birthday', 53: 'bitch.', 54: 'black', 55: 'blood', 56: 'bottle.', 57: 'bottom', 58: 'brain', 59: 'breaker.', 60: 'bus', 61: 'but', 62: 'calls', 63: 'can', 64: 'care', 65: 'catatonic.', 66: 'center', 67: 'chance', 68: 'circuit', 69: 'computer', 70: 'could', 71: 'covers', 72: 'crushes', 73: 'cry', 74: 'cuts', 75: 'days', 76: 'decapitates', 77: 'deity.', 78: 'discovering', 79: 'disproves', 80: 'do', 81: 'does', 82: \"don't\", 83: 'eat', 84: 'eats', 85: 'every', 86: 'example,', 87: 'flashlight', 88: 'for', 89: 'free', 90: 'genitals,', 91: 'genitals.', 92: 'get', 93: 'ghost', 94: 'girlfriend', 95: 'gravity,', 96: 'had', 97: 'hand.', 98: 'has,', 99: 'have', 100: 'have?', 101: 'having', 102: 'heartless', 103: 'here', 104: 'hole', 105: 'humans', 106: 'if', 107: 'impairment;', 108: 'in', 109: 'insane,', 110: 'insects', 111: 'involves', 112: 'is', 113: \"isn't\", 114: 'it', 115: 'it.', 116: 'just', 117: 'kept', 118: 'knocks)', 119: 'later,', 120: 'little', 121: 'living', 122: 'lizard', 123: 'lizard,', 124: 'loud', 125: 'makes', 126: 'man', 127: 'masturbating', 128: 'me', 129: 'memory', 130: 'messy,', 131: 'money.', 132: 'moon-pie', 133: 'mother', 134: 'moved', 135: 'much', 136: 'must', 137: 'my', 138: 'next', 139: 'not', 140: 'nummy-nummy', 141: 'of', 142: 'on', 143: 'one.', 144: 'other', 145: 'others', 146: 'paper', 147: 'paper,', 148: 'people', 149: 'please', 150: 'poisons', 151: 'present', 152: 'prize', 153: 'relationship', 154: 'render', 155: 'reproduce', 156: 'right', 157: 'rock', 158: 'rock,', 159: 'rushed', 160: 'sad.', 161: 'say', 162: 'scissors', 163: 'scissors,', 164: 'scissors.', 165: 'searching', 166: 'sexual', 167: 'she', 168: 'smashes', 169: 'so', 170: 'sooner', 171: 'stopping', 172: 'stupid,', 173: 'taken', 174: 'telescope', 175: 'tested.', 176: 'that', 177: 'the', 178: 'things', 179: 'think', 180: 'thou', 181: 'three', 182: 'to', 183: 'today', 184: 'town.', 185: 'tried', 186: 'unnecessary', 187: 'unsanitary', 188: 'up.', 189: 'used', 190: 'usually', 191: 'vaporizes', 192: 'vodka', 193: 'way', 194: 'we', 195: 'well,', 196: 'which', 197: 'white', 198: 'will', 199: 'with', 200: 'women,', 201: 'would', 202: 'years,', 203: 'you', 204: 'your', 0: 'UKN'}\n",
            "\n",
            "Word to Index-  {'(3': 1, 'Ah,': 2, \"Amy's\": 3, 'And': 4, 'Explorer': 5, 'Firefox.': 6, 'For': 7, 'Galileo,': 8, 'Goblin': 9, 'Green': 10, 'Hubble': 11, 'I': 12, \"I'm\": 13, 'Internet': 14, 'Ladybugs': 15, 'Oh': 16, 'Paul': 17, 'Penny': 18, 'Penny!': 19, 'Pope': 20, 'Scissors': 21, 'She': 22, 'Spider-Man,': 23, 'Spock': 24, 'Spock,': 25, 'Thankfully': 26, 'The': 27, 'Two': 28, 'V': 29, 'Well,': 30, 'What': 31, 'Wheaton!': 32, 'Wil': 33, \"You're\": 34, 'a': 35, 'afraid': 36, 'all': 37, 'always': 38, 'am': 39, 'and': 40, 'appeals': 41, 'are': 42, 'art': 43, 'as': 44, 'at': 45, 'aware': 46, 'based': 47, 'be': 48, 'became': 49, 'because': 50, 'been': 51, 'birthday': 52, 'bitch.': 53, 'black': 54, 'blood': 55, 'bottle.': 56, 'bottom': 57, 'brain': 58, 'breaker.': 59, 'bus': 60, 'but': 61, 'calls': 62, 'can': 63, 'care': 64, 'catatonic.': 65, 'center': 66, 'chance': 67, 'circuit': 68, 'computer': 69, 'could': 70, 'covers': 71, 'crushes': 72, 'cry': 73, 'cuts': 74, 'days': 75, 'decapitates': 76, 'deity.': 77, 'discovering': 78, 'disproves': 79, 'do': 80, 'does': 81, \"don't\": 82, 'eat': 83, 'eats': 84, 'every': 85, 'example,': 86, 'flashlight': 87, 'for': 88, 'free': 89, 'genitals,': 90, 'genitals.': 91, 'get': 92, 'ghost': 93, 'girlfriend': 94, 'gravity,': 95, 'had': 96, 'hand.': 97, 'has,': 98, 'have': 99, 'have?': 100, 'having': 101, 'heartless': 102, 'here': 103, 'hole': 104, 'humans': 105, 'if': 106, 'impairment;': 107, 'in': 108, 'insane,': 109, 'insects': 110, 'involves': 111, 'is': 112, \"isn't\": 113, 'it': 114, 'it.': 115, 'just': 116, 'kept': 117, 'knocks)': 118, 'later,': 119, 'little': 120, 'living': 121, 'lizard': 122, 'lizard,': 123, 'loud': 124, 'makes': 125, 'man': 126, 'masturbating': 127, 'me': 128, 'memory': 129, 'messy,': 130, 'money.': 131, 'moon-pie': 132, 'mother': 133, 'moved': 134, 'much': 135, 'must': 136, 'my': 137, 'next': 138, 'not': 139, 'nummy-nummy': 140, 'of': 141, 'on': 142, 'one.': 143, 'other': 144, 'others': 145, 'paper': 146, 'paper,': 147, 'people': 148, 'please': 149, 'poisons': 150, 'present': 151, 'prize': 152, 'relationship': 153, 'render': 154, 'reproduce': 155, 'right': 156, 'rock': 157, 'rock,': 158, 'rushed': 159, 'sad.': 160, 'say': 161, 'scissors': 162, 'scissors,': 163, 'scissors.': 164, 'searching': 165, 'sexual': 166, 'she': 167, 'smashes': 168, 'so': 169, 'sooner': 170, 'stopping': 171, 'stupid,': 172, 'taken': 173, 'telescope': 174, 'tested.': 175, 'that': 176, 'the': 177, 'things': 178, 'think': 179, 'thou': 180, 'three': 181, 'to': 182, 'today': 183, 'town.': 184, 'tried': 185, 'unnecessary': 186, 'unsanitary': 187, 'up.': 188, 'used': 189, 'usually': 190, 'vaporizes': 191, 'vodka': 192, 'way': 193, 'we': 194, 'well,': 195, 'which': 196, 'white': 197, 'will': 198, 'with': 199, 'women,': 200, 'would': 201, 'years,': 202, 'you': 203, 'your': 204, 'UKN': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjO5JqIoADq5",
        "colab_type": "text"
      },
      "source": [
        "Cool, This is the first step in building language models! You extracted the vocabulary unique_words of the raw texts and created dictionaries to go from words to numerical indexes and vice versa. Let's continue!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j0JcD0gBBbX",
        "colab_type": "text"
      },
      "source": [
        "#### Step2: Preparing text data for model input\n",
        "Previously, you learned how to create dictionaries of indexes to words and vice versa. In this exercise, you will split the text by characters and continue to prepare the data for supervised learning.\n",
        "\n",
        "Splitting the texts into characters may seem strange, but it is often done for text generation. Also, the process to prepare the data is the same, the only change is how to split the texts.\n",
        "\n",
        "You will create the training data containing a list of fixed-length texts and their labels, which are the corresponding next characters.\n",
        "\n",
        "You will continue to use the dataset containing quotes from Sheldon (The Big Bang Theory), available in the sheldon_quotes variable.\n",
        "\n",
        "The print_examples() function print the pairs so you can see how the data was transformed. Use help() for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bxB9ROBHsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a1783755-2884-4aed-a2db-f684c7f74821"
      },
      "source": [
        "# Create lists to keep the sentences and the next character\n",
        "sentences = []   # ~ Training data\n",
        "next_chars = []  # ~ Training labels\n",
        "\n",
        "# Define hyperparameters\n",
        "step = 2          # ~ Step to take when reading the texts in characters\n",
        "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
        "\n",
        "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
        "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
        "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
        "    next_chars.append(sheldon_quotes[i+chars_window])\n",
        "\n",
        "# Print 10 pairs\n",
        "print(sentences, next_chars)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"You're afraid of insects and women, Ladybugs must render you catatonic.\", 'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.', 'For example, I cry because others are stupid, and that makes me sad.', \"I'm not insane, my mother had me tested.\", 'Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.', \"Amy's birthday present will be my genitals.\", '(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!', 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.', 'I would have been here sooner but the bus kept stopping for other people to get on it.', 'Oh gravity, thou art a heartless bitch.'], ['For example, I cry because others are stupid, and that makes me sad.', \"I'm not insane, my mother had me tested.\", 'Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.', \"Amy's birthday present will be my genitals.\", '(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!', 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.', 'I would have been here sooner but the bus kept stopping for other people to get on it.', 'Oh gravity, thou art a heartless bitch.', 'I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.', 'Well, today we tried masturbating for money.'], ['Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.', \"Amy's birthday present will be my genitals.\", '(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!', 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.', 'I would have been here sooner but the bus kept stopping for other people to get on it.', 'Oh gravity, thou art a heartless bitch.', 'I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.', 'Well, today we tried masturbating for money.', 'I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.', \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\"], ['(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!', 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.', 'I would have been here sooner but the bus kept stopping for other people to get on it.', 'Oh gravity, thou art a heartless bitch.', 'I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.', 'Well, today we tried masturbating for money.', 'I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.', \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\", \"What computer do you have? And please don't say a white one.\", \"She calls me moon-pie because I'm nummy-nummy and she could just eat me up.\"]] ['I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.', 'I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.', \"What computer do you have? And please don't say a white one.\", 'Ah, memory impairment; the free prize at the bottom of every vodka bottle.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUsOtP5WRmIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c8faf648-6028-4587-9917-fcf32ba05654"
      },
      "source": [
        "sheldon_quotes = \"You're afraid of insects and women, Ladybugs must render you catatonic.Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.For example, I cry because others are stupid, and that makes me sad.I'm not insane, my mother had me tested.Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.Amy's birthday present will be my genitals.(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!Thankfully all the things my girlfriend used to do can be taken care of with my right hand.I would have been here sooner but the bus kept stopping for other people to get on it.Oh gravity, thou art a heartless bitch.I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.Well, today we tried masturbating for money.I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.What computer do you have? And please don't say a white one.She calls me moon-pie because I'm nummy-nummy and she could just eat me up.Ah, memory impairment; the free prize at the bottom of every vodka bottle.\"\n",
        "\n",
        "# Create lists to keep the sentences and the next character\n",
        "sentences = []   # ~ Training data\n",
        "next_chars = []  # ~ Training labels\n",
        "\n",
        "# Define hyperparameters\n",
        "step = 2          # ~ Step to take when reading the texts in characters\n",
        "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
        "\n",
        "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
        "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
        "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
        "    next_chars.append(sheldon_quotes[i+chars_window])\n",
        "\n",
        "pairs = (list(zip(sentences, next_chars))[:10])\n",
        "print(\"{}\\t\\t{}\".format(\"Sentence\", \"Next Char\"))\n",
        "for i in range(0,10):\n",
        "  print(\"{}\\t\\t{}\".format(pairs[i][0], pairs[i][1]))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence\t\tNext Char\n",
            "You're afr\t\ta\n",
            "u're afrai\t\td\n",
            "re afraid \t\to\n",
            " afraid of\t\t \n",
            "fraid of i\t\tn\n",
            "aid of ins\t\te\n",
            "d of insec\t\tt\n",
            "of insects\t\t \n",
            " insects a\t\tn\n",
            "nsects and\t\t \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNeLbhGR7WL",
        "colab_type": "text"
      },
      "source": [
        "With this you are ready to use the sentences and next character to train a supervised learning model! Don't mind that the printed sentences look strange, since you used characters instead of words and defined a sentence with a fixed length, the texts can be broken in the middle of a word. Note that the process of creating the sentences and next chars is the same when using words instead of characters, the only change being the values present on the lists (words instead of characters). Now, before going straight to training machine learning models, let's see what to do when you have a new text data not pre-processed yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XBBWaHzXv2Y",
        "colab_type": "text"
      },
      "source": [
        "### Transforming new text\n",
        "In this exercise, you will transform a new text into sequences of numerical indexes on the dictionaries created before.\n",
        "\n",
        "This is useful when you already have a trained model and want to apply it on a new dataset. The preprocessing steps done on the training data should also be applied to the new text, so the model can make predictions/classifications.\n",
        "\n",
        "Here, you will also use a special token '<UKN/>' to represent words that are not in the vocabulary. Typically, these special tokens are the first indexes of the dictionaries, the position 0.\n",
        "\n",
        "The variables word_to_index, index_to_word and vocabulary are already loaded in the environment. Also, the variable with the new text is also loaded as new_text. The new text has been printed for you to have a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ikQDFtSBNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_text = ['A man either lives life as it happens to him meets it head-on and licks it or he turns his back on it and starts to wither away', 'To the brave crew and passengers of the Kobayshi Maru sucks to be you', 'Beware of more powerful weapons They often inflict as much damage to your soul as they do to you enemies', 'They are merely scars not mortal wounds and you must use them to propel you forward', 'You cannot explain away a wantonly immoral act because you think that it is connected to some higher purpose']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjVwGI-XXvNi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "14920f58-3f75-4e08-f4d8-292fcc0aa456"
      },
      "source": [
        "# Loop through the sentences and get indexes\n",
        "new_text_split = []\n",
        "for sentence in new_text:\n",
        "    sent_split = []\n",
        "    for wd in sentence.split(' '):\n",
        "        index = word_to_index.get(wd,0)\n",
        "        sent_split.append(index)\n",
        "    new_text_split.append(sent_split)\n",
        "\n",
        "# Print the first sentence's indexes\n",
        "print(new_text_split[0])\n",
        "\n",
        "# Print the sentence converted using the dictionary\n",
        "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 126, 0, 0, 0, 44, 114, 0, 182, 0, 0, 114, 0, 40, 0, 114, 0, 0, 0, 0, 0, 142, 114, 40, 0, 182, 0, 0]\n",
            "UKN man UKN UKN UKN as it UKN to UKN UKN it UKN and UKN it UKN UKN UKN UKN UKN on it and UKN to UKN UKN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofnql28xY6nf",
        "colab_type": "text"
      },
      "source": [
        "You can see that some of the words were not found on the dictionary and have index = 0. By using the token 'UKN' in the training phase, you can easily use the model on unseen data without getting errors. This is also done when limiting the size of the vocabulary, say to 5,000 most frequent words, and setting the others as 'UKN'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5GsnjWSZQF8",
        "colab_type": "text"
      },
      "source": [
        "### Keras models\n",
        "In this exercise you'll practice using two classes from the keras.models module. You will create one model using the two classes Sequential and Model.\n",
        "\n",
        "The Sequential class is easier since the layers are assumed to be in order, while the Model class is more flexible and allows multiple inputs, multiple outputs and shared layers (shared weights).\n",
        "\n",
        "The Model class needs to explicitly declare the input layer, while in the Sequential class, this is done with the input_shape parameter.\n",
        "\n",
        "The objects and modules Sequential, Model, Dense, Input, LSTM and np (numpy) are already loaded on the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf4UqXE-Kmp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense,LSTM,Input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUAFfaJpLZTS",
        "colab_type": "text"
      },
      "source": [
        "**Sequential Class in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JufTQv8FKtQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "351a0272-e195-427d-bc66-df61d1b13627"
      },
      "source": [
        "# Instantiate Sequential class\n",
        "model = Sequential(name=\"sequential_model\")\n",
        "\n",
        "# One LSTM layer (defining the input shape because it is the \n",
        "# initial layer)\n",
        "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
        "\n",
        "# Add a dense layer with one unit\n",
        "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
        "\n",
        "# The summary shows the layers and the number of parameters \n",
        "# that will be trained\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "LSTM (LSTM)                  (None, 128)               71168     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 71,297\n",
            "Trainable params: 71,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5EmiafgLOch",
        "colab_type": "text"
      },
      "source": [
        "**Model Class in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpjRybyKK_Dq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "605d64b6-fbc1-46af-a1c2-0f97cfc25a18"
      },
      "source": [
        "# Define the input layer\n",
        "main_input = Input(shape=(None, 10), name=\"input\")\n",
        "\n",
        "# One LSTM layer (input shape is already defined)\n",
        "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
        "\n",
        "# Add a dense layer with one unit\n",
        "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
        "\n",
        "# Instantiate the Model class at the end\n",
        "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
        "\n",
        "# Same amount of parameters to train as before (71,297)\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"modelclass_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 10)          0         \n",
            "_________________________________________________________________\n",
            "LSTM (LSTM)                  (None, 128)               71168     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 71,297\n",
            "Trainable params: 71,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25zynTCoLHfy",
        "colab_type": "text"
      },
      "source": [
        "Great! You can see that the keras.models.Sequential is very easy to use to add layers in sequence. On the other hand, the keras.models.Model class is very flexible and is usually the choice when scientists need deep customization in their solution. Also, you saw how one layer is connected to another layer in both cases, be by adding them in sequence using the method add, or by creating a layer and calling the desired (previous) layer like a function, in the Model class API, every layer is callable on a tensor and always return a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcq41npkLNVx",
        "colab_type": "text"
      },
      "source": [
        "### Keras preprocessing\n",
        "The second most important module of Keras is keras.preprocessing. You will see how to use the most important modules and functions to prepare raw data to the correct input shape. Keras provides functionalities that substitute the dictionary approach you learned before.\n",
        "\n",
        "You will use the module keras.preprocessing.text.Tokenizer to create a dictionary of words using the method .fit_on_texts() and change the texts into numerical ids representing the index of each word on the dictionary using the method .texts_to_sequences().\n",
        "\n",
        "Then, use the function .pad_sequences() from keras.preprocessing.sequence to make all the sequences have the same size (necessary for the model) by adding zeros on the small texts and cutting the big ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9C7EdzZMTUi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3714163-c69e-43c7-9d50-1c6ac9fad95a"
      },
      "source": [
        "texts = ['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
        "       'Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?']\n",
        "len(texts)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_whYkWFLlMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "5a228939-d7f6-46da-e7c8-ebb96245643b"
      },
      "source": [
        "# Import relevant classes/functions\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts[0]), len(texts[1])))\n",
        "# Build the dictionary of indexes\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Change texts into sequence of indexes\n",
        "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
        "print(\"Number of words in the tokenized texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
        "\n",
        "# Pad the sequences\n",
        "texts_pad = pad_sequences(texts_numeric, 60)\n",
        "print(\"Number of words in the padded texts: ({0}, {1})\".format(len(texts_pad[0]), len(texts_pad[1])))\n",
        "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the sample texts: (279, 419)\n",
            "Number of words in the tokenized texts: (54, 78)\n",
            "Number of words in the padded texts: (60, 60)\n",
            "Now the texts have fixed length: 60. Let's see the first one: \n",
            "[ 0  0  0  0  0  0 24  4  1 25 13 26  5  1 14  3 27  6 28  2  7 29 30 13\n",
            " 15  2  8 16 17  5 18  6  4  9 31  2  8 32  4  9 15 33  9 34 35 14 36 37\n",
            "  2 38 39 40  2  8 16 41 42  5 18  6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqHbt1wwMJHa",
        "colab_type": "text"
      },
      "source": [
        "These functions are very useful to prepare raw texts to be inputted on RNN models. Next you are going to put everything together to classify sentiment on movie reviews!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkPtG6MtMsQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}